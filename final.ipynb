{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c987549a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>income_poverty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_group  education  income_poverty\n",
       "0          3        0.0             0.0\n",
       "1          1        1.0             0.0\n",
       "2          0        3.0             1.0\n",
       "3          4        1.0             0.0\n",
       "4          2        2.0             1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "df = pd.read_csv(\"training_set_features.csv\")\n",
    "\n",
    "\n",
    "# Define ordinal encoding mappings\n",
    "age_group_mapping = {'18 - 34 Years': 0, '35 - 44 Years': 1, '45 - 54 Years': 2, '55 - 64 Years': 3, '65+ Years': 4}\n",
    "education_mapping = {'< 12 Years': 0, '12 Years': 1, 'Some College': 2, 'College Graduate': 3}\n",
    "income_poverty_mapping = {'Below Poverty': 0, '<= $75,000, Above Poverty': 1, '> $75,000': 2}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "df['age_group'] = df['age_group'].map(age_group_mapping)\n",
    "df['education'] = df['education'].map(education_mapping)\n",
    "df['income_poverty'] = df['income_poverty'].map(income_poverty_mapping)\n",
    "\n",
    "# Impute missing values using KNNImputer\n",
    "columns_to_impute = ['age_group', 'education', 'income_poverty']\n",
    "\n",
    "# Perform MICE imputation\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df[columns_to_impute]), columns=columns_to_impute)\n",
    "df[['age_group','education','income_poverty']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487052bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "encoder = OneHotEncoder(drop='if_binary', sparse=False)\n",
    "X_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Construct dataframe with encoded columns\n",
    "encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Combine encoded dataframe with non-categorical columns\n",
    "df_encoded = pd.concat([df.drop(columns=categorical_columns), encoded_df], axis=1)\n",
    "\n",
    "# Perform MICE imputation\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_encoded), columns=df_encoded.columns)\n",
    "\n",
    "# Now df_imputed contains the modified original columns with imputed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1fcc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "respondent_id                     0\n",
       "h1n1_concern                      0\n",
       "h1n1_knowledge                    0\n",
       "behavioral_antiviral_meds         0\n",
       "behavioral_avoidance              0\n",
       "                                 ..\n",
       "employment_occupation_xgwztkwe    0\n",
       "employment_occupation_xqwwgdyp    0\n",
       "employment_occupation_xtkaffoo    0\n",
       "employment_occupation_xzmlyyjv    0\n",
       "employment_occupation_nan         0\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbba5bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: household_adults\n",
      "Unique values: [0. 2. 1. 3.]\n",
      "Number of missing values: 249\n",
      "Column: household_children\n",
      "Unique values: [0. 3. 2. 1.]\n",
      "Number of missing values: 249\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "df = pd.read_csv(\"training_set_features.csv\")\n",
    "for column in df[['household_adults', 'household_children']]:\n",
    "    unique_values = df[column].dropna().unique()  # Drop missing values before finding unique values\n",
    "    missing_values = df[column].isnull().sum()\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Unique values: {unique_values}\")\n",
    "    print(f\"Number of missing values: {missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cad0a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: respondent_id\n",
      "Unique values: [    0     1     2 ... 26704 26705 26706]\n",
      "no.of unique:  356618571\n",
      "--------------------\n",
      "Column: h1n1_vaccine\n",
      "Unique values: [0 1]\n",
      "no.of unique:  1\n",
      "--------------------\n",
      "Column: seasonal_vaccine\n",
      "Unique values: [0 1]\n",
      "no.of unique:  1\n",
      "--------------------\n",
      "seasonal_vaccine\n",
      "0    14272\n",
      "1    12435\n",
      "Name: count, dtype: int64\n",
      "h1n1_vaccine\n",
      "0    21033\n",
      "1     5674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "y = pd.read_csv(\"training_set_labels.csv\")\n",
    "for col in y:\n",
    "    unique_values = y[col].unique()\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Unique values: {unique_values}\")\n",
    "    print(\"no.of unique: \", y[col].unique().sum())\n",
    "    print(\"-\" * 20)\n",
    "print(y['seasonal_vaccine'].value_counts())\n",
    "print(y['h1n1_vaccine'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the datasets\n",
    "X = pd.read_csv(\"training_set_features.csv\")\n",
    "X = X.drop(['employment_industry', 'employment_occupation', 'respondent_id'], axis=1)\n",
    "y = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'seasonal_vaccine'], axis=1)\n",
    "z = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'h1n1_vaccine'], axis=1)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing steps\n",
    "numerical_transformer = SimpleImputer(strategy='median')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define parameter search space\n",
    "param_space = {\n",
    "    'model__base_estimator__n_estimators': [225, 200, 250, 275, 212],\n",
    "    'model__base_estimator__max_depth': [3, 8, 9, 10, 11],\n",
    "    'model__base_estimator__learning_rate': [0.155, 0.18, 0.16, 0.15, 0.17]\n",
    "}\n",
    "\n",
    "# Define stratified k-fold cross-validator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to perform hyperparameter tuning, k-fold cross-validation, and evaluation\n",
    "def evaluate_model(X_data, y_data):\n",
    "    # Compute scale_pos_weight based on the class distribution in the training data\n",
    "    scale_pos_weight = (len(y_data) - y_data.sum()) / y_data.sum()\n",
    "    scale_pos_weight = scale_pos_weight.iloc[0]\n",
    "\n",
    "    # Define the XGBoost model with scale_pos_weight\n",
    "    model_xgb = XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "    # Define the BaggingClassifier with XGBClassifier as the base estimator\n",
    "    model_bagging = BaggingClassifier(base_estimator=model_xgb, n_estimators=10)\n",
    "\n",
    "    # Create pipeline with preprocessing and modeling steps\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('model', model_bagging)])\n",
    "\n",
    "    # Perform hyperparameter tuning with RandomizedSearchCV and stratified k-fold cross-validation\n",
    "    random_search = RandomizedSearchCV(\n",
    "        clf,\n",
    "        param_distributions=param_space,\n",
    "        n_iter=10,\n",
    "        cv=skf,\n",
    "        scoring='accuracy',\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available CPU cores for parallel computation\n",
    "    )\n",
    "\n",
    "    # Fit the model using RandomizedSearchCV\n",
    "    random_search.fit(X_data, y_data)\n",
    "\n",
    "    # Print best hyperparameters\n",
    "    print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "    print()\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Perform stratified k-fold testing with the best model\n",
    "    accuracy_scores = []\n",
    "    classification_reports = []\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(total=skf.get_n_splits(X_data, y_data)) as pbar:\n",
    "        for train_index, test_index in skf.split(X_data, y_data):\n",
    "            X_train_skf, X_test_skf = X_data.iloc[train_index], X_data.iloc[test_index]\n",
    "            y_train_skf, y_test_skf = y_data.iloc[train_index], y_data.iloc[test_index]\n",
    "\n",
    "            # Fit the best model\n",
    "            best_model.fit(X_train_skf, y_train_skf)\n",
    "\n",
    "            # Predict on the test set\n",
    "            y_pred_skf = best_model.predict(X_test_skf)\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy = accuracy_score(y_test_skf, y_pred_skf)\n",
    "            accuracy_scores.append(accuracy)\n",
    "\n",
    "            classification_rep = classification_report(y_test_skf, y_pred_skf)\n",
    "            classification_reports.append(classification_rep)\n",
    "\n",
    "            y_pred_proba = best_model.predict_proba(X_test_skf)[:, 1]  # Probability of positive class\n",
    "            roc_auc = roc_auc_score(y_test_skf, y_pred_proba)\n",
    "            roc_auc_scores.append(roc_auc)\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate mean scores across all fold\n",
    "    mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "    mean_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
    "\n",
    "    print(\"Mean Accuracy:\", mean_accuracy)\n",
    "    print(\"Mean ROC AUC Score:\", mean_roc_auc)\n",
    "\n",
    "    # Print classification reports for each fold\n",
    "    for i, report in enumerate(classification_reports):\n",
    "        print(f\"\\nClassification Report for Fold {i+1}:\\n{report}\")\n",
    "\n",
    "# Evaluate model for dataset y\n",
    "print(\"Evaluation for Dataset y:\")\n",
    "evaluate_model(X, y)\n",
    "\n",
    "# Evalu ate model for dataset z\n",
    "print(\"\\nEvaluation for Dataset z:\")\n",
    "evaluate_model(X, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208131e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.54049 | val_0_auc: 0.68342 |  0:00:02s\n",
      "epoch 1  | loss: 0.47489 | val_0_auc: 0.73437 |  0:00:04s\n",
      "epoch 2  | loss: 0.45555 | val_0_auc: 0.76749 |  0:00:07s\n",
      "epoch 3  | loss: 0.44236 | val_0_auc: 0.77983 |  0:00:09s\n",
      "epoch 4  | loss: 0.43623 | val_0_auc: 0.78652 |  0:00:12s\n",
      "epoch 5  | loss: 0.42295 | val_0_auc: 0.79896 |  0:00:14s\n",
      "epoch 6  | loss: 0.41681 | val_0_auc: 0.80147 |  0:00:16s\n",
      "epoch 7  | loss: 0.41475 | val_0_auc: 0.80394 |  0:00:18s\n",
      "epoch 8  | loss: 0.40443 | val_0_auc: 0.81185 |  0:00:20s\n",
      "epoch 9  | loss: 0.40108 | val_0_auc: 0.81801 |  0:00:22s\n",
      "epoch 10 | loss: 0.39979 | val_0_auc: 0.81705 |  0:00:25s\n",
      "epoch 11 | loss: 0.39696 | val_0_auc: 0.82108 |  0:00:28s\n",
      "epoch 12 | loss: 0.39578 | val_0_auc: 0.82096 |  0:00:30s\n",
      "epoch 13 | loss: 0.3943  | val_0_auc: 0.82103 |  0:00:32s\n",
      "epoch 14 | loss: 0.39261 | val_0_auc: 0.82081 |  0:00:34s\n",
      "epoch 15 | loss: 0.39109 | val_0_auc: 0.82115 |  0:00:37s\n",
      "epoch 16 | loss: 0.38851 | val_0_auc: 0.81761 |  0:00:39s\n",
      "epoch 17 | loss: 0.38574 | val_0_auc: 0.82312 |  0:00:42s\n",
      "epoch 18 | loss: 0.38593 | val_0_auc: 0.82061 |  0:00:45s\n",
      "epoch 19 | loss: 0.38325 | val_0_auc: 0.81929 |  0:00:47s\n",
      "epoch 20 | loss: 0.3811  | val_0_auc: 0.82097 |  0:00:49s\n",
      "epoch 21 | loss: 0.38281 | val_0_auc: 0.8259  |  0:00:52s\n",
      "epoch 22 | loss: 0.38262 | val_0_auc: 0.82281 |  0:00:54s\n",
      "epoch 23 | loss: 0.38208 | val_0_auc: 0.82186 |  0:00:57s\n",
      "epoch 24 | loss: 0.38052 | val_0_auc: 0.82202 |  0:01:00s\n",
      "epoch 25 | loss: 0.38194 | val_0_auc: 0.81931 |  0:01:03s\n",
      "epoch 26 | loss: 0.37833 | val_0_auc: 0.8172  |  0:01:06s\n",
      "epoch 27 | loss: 0.37687 | val_0_auc: 0.82155 |  0:01:08s\n",
      "epoch 28 | loss: 0.37473 | val_0_auc: 0.81941 |  0:01:10s\n",
      "epoch 29 | loss: 0.37316 | val_0_auc: 0.81975 |  0:01:13s\n",
      "epoch 30 | loss: 0.37554 | val_0_auc: 0.82169 |  0:01:16s\n",
      "epoch 31 | loss: 0.37169 | val_0_auc: 0.81913 |  0:01:18s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.8259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8382628229127668\n",
      "ROC AUC Score: 0.6944324685475127\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90      4212\n",
      "           1       0.68      0.45      0.54      1130\n",
      "\n",
      "    accuracy                           0.84      5342\n",
      "   macro avg       0.77      0.69      0.72      5342\n",
      "weighted avg       0.82      0.84      0.82      5342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Load the datasets\n",
    "X = pd.read_csv(\"training_set_features.csv\")\n",
    "X = X.drop(['employment_industry', 'employment_occupation','respondent_id'], axis=1)\n",
    "y = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'seasonal_vaccine'], axis=1)\n",
    "\n",
    "# Preprocessing\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess data using the pipeline\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Define TabNet model\n",
    "tabnet_model = TabNetClassifier()\n",
    "\n",
    "# Train the model\n",
    "tabnet_model.fit(X_train, y_train.values.ravel(), eval_set=[(X_test, y_test.values.ravel())], patience=10)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = tabnet_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_score = roc_auc_score(y_test, y_pred)\n",
    "classification_report_text = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"ROC AUC Score:\", roc_score)\n",
    "print(\"Classification Report:\\n\", classification_report_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda667e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the datasets\n",
    "X = pd.read_csv(\"training_set_features.csv\")\n",
    "X = X.drop(['employment_industry', 'employment_occupation', 'respondent_id', 'health_insurance'], axis=1)\n",
    "y = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'seasonal_vaccine'], axis=1)\n",
    "z = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'h1n1_vaccine'], axis=1)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pipeline for preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(random_state=42, max_iter=50))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', XGBClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7517726c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m numerical_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     22\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m---> 24\u001b[0m numerical_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     25\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     26\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler())\n\u001b[0;32m     27\u001b[0m ])\n\u001b[0;32m     29\u001b[0m categorical_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     30\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     31\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     32\u001b[0m ])\n\u001b[0;32m     34\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m     35\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     36\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m, numerical_transformer, numerical_features),\n\u001b[0;32m     37\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_transformer, categorical_features)\n\u001b[0;32m     38\u001b[0m     ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tabnet import TabNetClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load data\n",
    "X = pd.read_csv(\"training_set_features.csv\")\n",
    "X.drop(['employment_industry', 'employment_occupation', 'respondent_id', 'health_insurance'], axis=1, inplace=True)\n",
    "y = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id'], axis=1)  # Assuming both 'h1n1_vaccine' and 'seasonal_vaccine' are target variables\n",
    "\n",
    "# Separate target variables\n",
    "y_h1n1 = y['h1n1_vaccine']\n",
    "y_seasonal = y['seasonal_vaccine']\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Oversampling for class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning with regularization\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {\n",
    "    'feature_dim': [32, 64, 128],  # Adjust based on feature importance and computational resources\n",
    "    'output_dim': 2,  # Number of target variables\n",
    "    'n_decision_trees': [50, 100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'optimizer': ['adam', 'adamw'],\n",
    "    'scheduler': ['cosineannealinglr', 'reducelronplateau'],\n",
    "    'lambda_sparse': [0.001, 0.01, 0.1],\n",
    "    'seed': [42, 100]\n",
    "}\n",
    "model = TabNetClassifier(\n",
    "    cat_cols=categorical_features,  # Specify categorical features for regularization\n",
    "    cat_embed_dim=16  # Adjust embedding dimension for categorical features\n",
    ")\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='roc_auc_macro', cv=kfold)\n",
    "\n",
    "# Train model with hyperparameter tuning, regularization, and oversampling\n",
    "grid_search.fit(X_imputed, [y_h1n1, y_seasonal])\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Access best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Store metrics and reports\n",
    "mean_roc_h1n1, mean_roc_seasonal = 0, 0\n",
    "mean_accuracy_h1n1, mean_accuracy_seasonal = 0, 0\n",
    "classification_reports_h1n1, classification_reports_seasonal = [], []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X_imputed, [y_h1n1, y_seasonal])):\n",
    "    X_train, X_test = X_imputed[train_index], X_imputed[test_index]\n",
    "    y_train_h1n1, y_test_h1n1 = y_h1n1.iloc[train_index], y_test_h1n1\n",
    "    y_train_seasonal, y_test_seasonal = y_seasonal.iloc[train_index], y_test_seasonal\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Oversample training data for h1n1 and seasonal vaccines\n",
    "    X_train_resampled_h1n1, y_train_h1n1_resampled = smote.fit_resample(X_train_preprocessed, y_train_h1n1)\n",
    "    X_train_resampled_seasonal, y_train_seasonal_resampled = smote.fit_resample(X_train_preprocessed, y_train_seasonal)\n",
    "\n",
    "    # Train model on oversampled data (replace with undersampling if preferable)\n",
    "    best_model.fit(X_train_resampled_h1n1, y_train_h1n1_resampled, X_train_resampled_seasonal, y_train_seasonal_resampled)\n",
    "\n",
    "    # Prediction and evaluation\n",
    "    y_pred_h1n1, y_pred_prob_h1n1 = best_model.predict(X_test_preprocessed)\n",
    "    y_pred_seasonal, y_pred_prob_seasonal = best_model.predict(X_test_preprocessed)\n",
    "\n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc_h1n1 = roc_auc_score(y_test_h1n1, y_pred_prob_h1n1[:, 0])\n",
    "    roc_auc_seasonal = roc_auc_score(y_test_seasonal, y_pred_prob_seasonal[:, 1])\n",
    "    mean_roc_h1n1 += roc_auc_h1n1\n",
    "    mean_roc_seasonal += roc_auc_seasonal\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy_h1n1 = accuracy_score(y_test_h1n1, y_pred_h1n1)\n",
    "    accuracy_seasonal = accuracy_score(y_test_seasonal, y_pred_seasonal)\n",
    "    mean_accuracy_h1n1 += accuracy_h1n1\n",
    "    mean_accuracy_seasonal += accuracy_seasonal\n",
    "\n",
    "    # Generate classification reports\n",
    "    classification_reports_h1n1.append(classification_report(y_test_h1n1, y_pred_h1n1))\n",
    "    classification_reports_seasonal.append(classification_report(y_test_seasonal, y_pred_seasonal))\n",
    "\n",
    "# Print aggregated metrics\n",
    "print(\"Mean ROC AUC (h1n1):\", mean_roc_h1n1 / kfold.n_splits)\n",
    "print(\"Mean ROC AUC (seasonal):\", mean_roc_seasonal / kfold.n_splits)\n",
    "print(\"Mean accuracy (h1n1):\", mean_accuracy_h1n1 / kfold.n_splits)\n",
    "print(\"Mean accuracy (seasonal):\", mean_accuracy_seasonal / kfold.n_splits)\n",
    "\n",
    "# Print or save detailed classification reports\n",
    "print(\"Classification reports:\")\n",
    "for report in classification_reports_h1n1:\n",
    "    print(\"h1n1 vaccine:\")\n",
    "    print(report)\n",
    "for report in classification_reports_seasonal:\n",
    "    print(\"seasonal vaccine:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c6a9e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TabNetClassifier.__init__() missing 2 required positional arguments: 'feature_columns' and 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m\n\u001b[0;32m     44\u001b[0m kfold \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     45\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m],  \u001b[38;5;66;03m# Adjust based on feature importance and computational resources\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Number of target variables\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     55\u001b[0m }\n\u001b[1;32m---> 56\u001b[0m model \u001b[38;5;241m=\u001b[39m TabNetClassifier(\n\u001b[0;32m     57\u001b[0m     cat_cols\u001b[38;5;241m=\u001b[39mcategorical_features,  \u001b[38;5;66;03m# Specify categorical features for regularization\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     cat_embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m  \u001b[38;5;66;03m# Adjust embedding dimension for categorical features\u001b[39;00m\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc_macro\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mkfold)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Train model with hyperparameter tuning, regularization, and oversampling\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TabNetClassifier.__init__() missing 2 required positional arguments: 'feature_columns' and 'num_classes'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tabnet import TabNetClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "from fancyimpute import IterativeImputer  # Importing IterativeImputer from fancyimpute\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load data\n",
    "X = pd.read_csv(\"training_set_features.csv\")\n",
    "X.drop(['employment_industry', 'employment_occupation', 'respondent_id', 'health_insurance'], axis=1, inplace=True)\n",
    "y = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id'], axis=1)  # Assuming both 'h1n1_vaccine' and 'seasonal_vaccine' are target variables\n",
    "\n",
    "# Separate target variables\n",
    "y_h1n1 = y['h1n1_vaccine']\n",
    "y_seasonal = y['seasonal_vaccine']\n",
    "\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Oversampling for class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning with regularization\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {\n",
    "    'feature_dim': [32, 64, 128],  # Adjust based on feature importance and computational resources\n",
    "    'output_dim': 2,  # Number of target variables\n",
    "    'n_decision_trees': [50, 100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'optimizer': ['adam', 'adamw'],\n",
    "    'scheduler': ['cosineannealinglr', 'reducelronplateau'],\n",
    "    'lambda_sparse': [0.001, 0.01, 0.1],\n",
    "    'seed': [42, 100]\n",
    "}\n",
    "model = TabNetClassifier(\n",
    "    cat_cols=categorical_features,  # Specify categorical features for regularization\n",
    "    cat_embed_dim=16  # Adjust embedding dimension for categorical features\n",
    ")\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='roc_auc_macro', cv=kfold)\n",
    "\n",
    "# Train model with hyperparameter tuning, regularization, and oversampling\n",
    "grid_search.fit(X_imputed, [y_h1n1, y_seasonal])\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Access best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Store metrics and reports\n",
    "mean_roc_h1n1, mean_roc_seasonal = 0, 0\n",
    "mean_accuracy_h1n1, mean_accuracy_seasonal = 0, 0\n",
    "classification_reports_h1n1, classification_reports_seasonal = [], []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X_imputed, [y_h1n1, y_seasonal])):\n",
    "    X_train, X_test = X_imputed[train_index], X_imputed[test_index]\n",
    "    y_train_h1n1, y_test_h1n1 = y_h1n1.iloc[train_index], y_h1n1.iloc[test_index]  # Fix variable names here\n",
    "    y_train_seasonal, y_test_seasonal = y_seasonal.iloc[train_index], y_seasonal.iloc[test_index]  # Fix variable names here\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Oversample training data for h1n1 and seasonal vaccines\n",
    "    X_train_resampled_h1n1, y_train_h1n1_resampled = smote.fit_resample(X_train_preprocessed, y_train_h1n1)\n",
    "    X_train_resampled_seasonal, y_train_seasonal_resampled = smote.fit_resample(X_train_preprocessed, y_train_seasonal)\n",
    "\n",
    "    # Train model on oversampled data (replace with undersampling if preferable)\n",
    "    best_model.fit(X_train_resampled_h1n1, y_train_h1n1_resampled, X_train_resampled_seasonal, y_train_seasonal_resampled)\n",
    "\n",
    "    # Prediction and evaluation\n",
    "    y_pred_h1n1, y_pred_prob_h1n1 = best_model.predict(X_test_preprocessed)\n",
    "    y_pred_seasonal, y_pred_prob_seasonal = best_model.predict(X_test_preprocessed)\n",
    "\n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc_h1n1 = roc_auc_score(y_test_h1n1, y_pred_prob_h1n1[:, 0])\n",
    "    roc_auc_seasonal = roc_auc_score(y_test_seasonal, y_pred_prob_seasonal[:, 1])\n",
    "    mean_roc_h1n1 += roc_auc_h1n1\n",
    "    mean_roc_seasonal += roc_auc_seasonal\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy_h1n1 = accuracy_score(y_test_h1n1, y_pred_h1n1)\n",
    "    accuracy_seasonal = accuracy_score(y_test_seasonal, y_pred_seasonal)\n",
    "    mean_accuracy_h1n1 += accuracy_h1n1\n",
    "    mean_accuracy_seasonal += accuracy_seasonal\n",
    "\n",
    "    # Generate classification reports\n",
    "    classification_reports_h1n1.append(classification_report(y_test_h1n1, y_pred_h1n1))\n",
    "    classification_reports_seasonal.append(classification_report(y_test_seasonal, y_pred_seasonal))\n",
    "\n",
    "# Print aggregated metrics\n",
    "print(\"Mean ROC AUC (h1n1):\", mean_roc_h1n1 / kfold.n_splits)\n",
    "print(\"Mean ROC AUC (seasonal):\", mean_roc_seasonal / kfold.n_splits)\n",
    "print(\"Mean accuracy (h1n1):\", mean_accuracy_h1n1 / kfold.n_splits)\n",
    "print(\"Mean accuracy (seasonal):\", mean_accuracy_seasonal / kfold.n_splits)\n",
    "\n",
    "# Print or save detailed classification reports\n",
    "print(\"Classification reports:\")\n",
    "for report in classification_reports_h1n1:\n",
    "    print(\"h1n1 vaccine:\")\n",
    "    print(report)\n",
    "for report in classification_reports_seasonal:\n",
    "    print(\"seasonal vaccine:\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac68651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TabModel.fit() got an unexpected keyword argument 'n_d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx], y\u001b[38;5;241m.\u001b[39miloc[test_idx]\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Train the TabNetClassifier model with early stopping\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, model__callbacks\u001b[38;5;241m=\u001b[39m[EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)], model__n_d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, model__n_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, model__n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, model__gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.3\u001b[39m, model__lambda_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, model__batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, model__virtual_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m     88\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\sklearn\\pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 475\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: TabModel.fit() got an unexpected keyword argument 'n_d'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Load the datasets\n",
    "X = pd.read_csv(\"training_set_features.csv\")\n",
    "X = X.drop(['employment_industry', 'employment_occupation', 'respondent_id', 'health_insurance'], axis=1)\n",
    "y = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'seasonal_vaccine'], axis=1)\n",
    "z = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'h1n1_vaccine'], axis=1)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pipeline for preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(random_state=42, max_iter=50)),\n",
    "    ('scaler', StandardScaler())  # Standardization\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define TabNetClassifier model\n",
    "cat_idxs = [X.columns.get_loc(col) for col in categorical_features]  # Define cat_idxs\n",
    "cat_dims = [len(X[col].unique()) for col in categorical_features]  # Define cat_dims\n",
    "\n",
    "model = TabNetClassifier(optimizer_params=dict(lr=2e-2, weight_decay=1e-5),  # Adjust weight_decay for class imbalance handling\n",
    "                         scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "                         scheduler_fn=\"StepLR\",\n",
    "                         verbose=0,\n",
    "                         cat_idxs=cat_idxs,  # Pass cat_idxs\n",
    "                         cat_dims=cat_dims,  # Pass cat_dims\n",
    "                         cat_emb_dim=1,\n",
    "                         mask_type=\"entmax\",\n",
    "                         device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         )\n",
    "\n",
    "# Combine preprocessing with TabNetClassifier model in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)])\n",
    "\n",
    "# Hyperparameters for tuning\n",
    "params = {\n",
    "    \"model__n_d\": [8, 16],\n",
    "    \"model__n_a\": [8, 16],\n",
    "    \"model__n_steps\": [3, 5],\n",
    "    \"model__gamma\": [1.3, 1.8],\n",
    "    \"model__lambda_sparse\": [0.0001, 0.001],\n",
    "    \"model__batch_size\": [64, 128],\n",
    "    \"model__virtual_batch_size\": [32, 64],\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "roc_auc_scores = []\n",
    "accuracy_scores = []\n",
    "classification_reports = []\n",
    "\n",
    "# Perform K-Fold Cross Validation with hyperparameter tuning\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold: {fold+1}\")\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train the TabNetClassifier model with early stopping\n",
    "    clf.fit(X_train, y_train, model__callbacks=[EarlyStopping(patience=10)], model__n_d=8, model__n_a=8, model__n_steps=5, model__gamma=1.3, model__lambda_sparse=0.001, model__batch_size=64, model__virtual_batch_size=32)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, (y_pred_proba > 0.5).astype(int))\n",
    "    classification_rep = classification_report(y_test, (y_pred_proba > 0.5).astype(int))\n",
    "\n",
    "    # Append evaluation metrics to lists\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    classification_reports.append(classification_rep)\n",
    "\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print mean evaluation metrics across all folds\n",
    "print(\"Mean ROC AUC:\", np.mean(roc_auc_scores))\n",
    "print(\"Mean Accuracy:\", np.mean(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb3ae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'virtual_batch_size' for estimator TabNetClassifier(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=0, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02, 'weight_decay': 1e-05}, scheduler_fn='StepLR', scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='sparsemax', input_dim=None, output_dim=None, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[]). Valid parameters are: ['cat_dims', 'cat_emb_dim', 'cat_idxs', 'clip_value', 'device_name', 'epsilon', 'gamma', 'grouped_features', 'input_dim', 'lambda_sparse', 'mask_type', 'momentum', 'n_a', 'n_d', 'n_indep_decoder', 'n_independent', 'n_shared', 'n_shared_decoder', 'n_steps', 'optimizer_fn', 'optimizer_params', 'output_dim', 'scheduler_fn', 'scheduler_params', 'seed', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\anaconda\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"C:\\anaconda\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\anaconda\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\anaconda\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\anaconda\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 883, in _fit_and_score\n    estimator = estimator.set_params(**clone(parameters, safe=False))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\anaconda\\Lib\\site-packages\\sklearn\\pipeline.py\", line 239, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"C:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 71, in _set_params\n    super().set_params(**params)\n  File \"C:\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 291, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"C:\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 279, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'virtual_batch_size' for estimator TabNetClassifier(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=0, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02, 'weight_decay': 1e-05}, scheduler_fn='StepLR', scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='sparsemax', input_dim=None, output_dim=None, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[]). Valid parameters are: ['cat_dims', 'cat_emb_dim', 'cat_idxs', 'clip_value', 'device_name', 'epsilon', 'gamma', 'grouped_features', 'input_dim', 'lambda_sparse', 'mask_type', 'momentum', 'n_a', 'n_d', 'n_indep_decoder', 'n_independent', 'n_shared', 'n_shared_decoder', 'n_steps', 'optimizer_fn', 'optimizer_params', 'output_dim', 'scheduler_fn', 'scheduler_params', 'seed', 'verbose'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(clf, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39mskf, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Perform Randomized Search Cross Validation\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Best parameters and their corresponding score\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1915\u001b[0m         ParameterSampler(\n\u001b[0;32m   1916\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1917\u001b[0m         )\n\u001b[0;32m   1918\u001b[0m     )\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'virtual_batch_size' for estimator TabNetClassifier(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=0, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02, 'weight_decay': 1e-05}, scheduler_fn='StepLR', scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='sparsemax', input_dim=None, output_dim=None, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[]). Valid parameters are: ['cat_dims', 'cat_emb_dim', 'cat_idxs', 'clip_value', 'device_name', 'epsilon', 'gamma', 'grouped_features', 'input_dim', 'lambda_sparse', 'mask_type', 'momentum', 'n_a', 'n_d', 'n_indep_decoder', 'n_independent', 'n_shared', 'n_shared_decoder', 'n_steps', 'optimizer_fn', 'optimizer_params', 'output_dim', 'scheduler_fn', 'scheduler_params', 'seed', 'verbose']."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the datasets\n",
    "X = pd.read_csv(\"training_set_features.csv\")\n",
    "X = X.drop(['employment_industry', 'employment_occupation', 'respondent_id', 'health_insurance'], axis=1)\n",
    "y = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'seasonal_vaccine'], axis=1)\n",
    "z = pd.read_csv(\"training_set_labels.csv\").drop(['respondent_id', 'h1n1_vaccine'], axis=1)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pipeline for preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(random_state=42, max_iter=50)),\n",
    "    ('scaler', StandardScaler())  # Standardization\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define TabNetClassifier model\n",
    "model = TabNetClassifier(optimizer_params=dict(lr=2e-2, weight_decay=1e-5),  # Adjust weight_decay for class imbalance handling\n",
    "                         scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "                         scheduler_fn=\"StepLR\",\n",
    "                         verbose=0,\n",
    "                         device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         )\n",
    "\n",
    "# Combine preprocessing with TabNetClassifier model in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)])\n",
    "\n",
    "# Hyperparameters for tuning\n",
    "param_dist = {\n",
    "    \"model__n_d\": [8, 16],\n",
    "    \"model__n_a\": [8, 16],\n",
    "    \"model__n_steps\": [3, 5],\n",
    "    \"model__gamma\": [1.3, 1.8],\n",
    "    \"model__lambda_sparse\": [0.0001, 0.001],\n",
    "    \"model__batch_size\": [64, 128],\n",
    "    \"model__virtual_batch_size\": [32, 64],\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Randomized Search Cross Validation\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=10, scoring='roc_auc', n_jobs=-1, cv=skf, verbose=1)\n",
    "\n",
    "# Perform Randomized Search Cross Validation\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best parameters and their corresponding score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best ROC AUC Score:\", random_search.best_score_)\n",
    "\n",
    "# Instantiate TabNetClassifier with best parameters\n",
    "best_model = TabNetClassifier(optimizer_params=dict(lr=2e-2, weight_decay=1e-5),  # Adjust weight_decay for class imbalance handling\n",
    "                              scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "                              scheduler_fn=\"StepLR\",\n",
    "                              verbose=0,\n",
    "                              device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                              **random_search.best_params_\n",
    "                             )\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "roc_auc_scores = []\n",
    "accuracy_scores = []\n",
    "classification_reports = []\n",
    "\n",
    "# Perform K-Fold Cross Validation with best parameters\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold: {fold+1}\")\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train the TabNetClassifier model with early stopping\n",
    "    best_model.fit(X_train, y_train, callbacks=[EarlyStopping(patience=10)])\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, (y_pred_proba > 0.5).astype(int))\n",
    "    classification_rep = classification_report(y_test, (y_pred_proba > 0.5).astype(int))\n",
    "\n",
    "    # Append evaluation metrics to lists\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    classification_reports.append(classification_rep)\n",
    "\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print mean evaluation metrics across all folds\n",
    "print(\"Mean ROC AUC:\", np.mean(roc_auc_scores))\n",
    "print(\"Mean Accuracy:\", np.mean(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0336894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
